{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7b8f05ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##initialize the Node\n",
    "import numpy as np\n",
    "\n",
    "class Node: \n",
    "    def __init__(self, \n",
    "                 feature =None, \n",
    "                 threshold=None, \n",
    "                 data_left= None,\n",
    "                 data_right=None, \n",
    "                 gain=None, \n",
    "                 value=None):\n",
    "        self.feature = feature, \n",
    "        self.threshold = threshold\n",
    "        self.data_left = data_left\n",
    "        self.data_right = data_right\n",
    "        self.gain = gain \n",
    "        self.value = value\n",
    "        \n",
    "\n",
    "#decision tree classificiton \n",
    "class DecisionTree: \n",
    "    def __init__(self,min_sample_split = 2, max_depth = 5): \n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root= None \n",
    "        \n",
    "        \n",
    "    def _entropy(s):\n",
    "        \"\"\"\n",
    "        :param s: list \n",
    "        :return: float, entropy value \n",
    "        \n",
    "        \"\"\"\n",
    "        percentage = np.bincount(np.array(s))/len(s)\n",
    "        \n",
    "        #calculate entropy \n",
    "        entropy = 0 \n",
    "        for pct in percentage: \n",
    "            if pct>0: \n",
    "                entropy += pct * np.log2(pct)\n",
    "                \n",
    "        return -entropy\n",
    "    \n",
    "    def information_gain(self, parent, left_child, right_child):\n",
    "        num_left = len(left_child)/len(parent)\n",
    "        num_right = len(right_child)/len(parent)\n",
    "        \n",
    "        information_gain = self.entropy(parent) - (num_left*(entropy(left_child)) + num_right*(entropy(right_child)))\n",
    "        \n",
    "        return information_gain\n",
    "    \n",
    "    \n",
    "    def best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array, features \n",
    "        y: np.array or list\n",
    "        returns dict\n",
    "        \"\"\"\n",
    "        best_split = {}\n",
    "        best_info_gain = -1 \n",
    "        n_rows, n_cols = X.shape\n",
    "        \n",
    "        for index in range(n_cols):\n",
    "            X_curr = X[:,index]\n",
    "            \n",
    "            for threshold in np.unique(X_curr):\n",
    "                \n",
    "                df = np.concatenate((X, y.reshape(1,-1)), axis=1)\n",
    "                df_left = np.array([row for row in df if row[index]<=threshold])\n",
    "                df_right = np.array([row for row in df if row[index]>threshold])\n",
    "                \n",
    "                if len(df_left)>0 and len(df_right)>0: \n",
    "                    y = df[:, -1]\n",
    "                    y_left = df_left[:, -1]\n",
    "                    y_right = df_right[:, -1]\n",
    "                    \n",
    "                    #calculate information gain from the split\n",
    "                    gain = self.information_gain(y, y_left, y_right)\n",
    "                    \n",
    "                    if gain>best_info_gain: \n",
    "                        best_split = {\n",
    "                            \"feature_index\": index, \n",
    "                            \"threshold\": threshold, \n",
    "                            \"df_left\":df_left, \n",
    "                            \"df_right\":df_right, \n",
    "                            \"gain\":gain\n",
    "                        }\n",
    "                        \n",
    "                        best_info_gain = gain \n",
    "                        \n",
    "        return best_split\n",
    "    \n",
    "    \n",
    "    def build(self, X, y, depth = 0):\n",
    "        \"\"\"\n",
    "        X: np.array, features \n",
    "        y: np.array, list or target \n",
    "        depth: int, current depth of tree\n",
    "        return: Node\n",
    "        \"\"\"\n",
    "        \n",
    "        n_rows, n_cols = X.shape\n",
    "        \n",
    "        #base case to see if this should be leaf node \n",
    "        if n_rows >= self.min_sample_split and depth<=self.max_depth:\n",
    "            \n",
    "            best = self.best_split(X,y)\n",
    "            \n",
    "            if best[\"gain\"]>0: \n",
    "                \n",
    "                left = self.build(\n",
    "                    X = best[\"df_left\"][:,:-1],\n",
    "                    y = best[\"df_left\"][:, -1], \n",
    "                    depth = depth+1\n",
    "                )\n",
    "                \n",
    "                right = self.build(\n",
    "                    X = best[\"df_right\"][:,:-1],\n",
    "                    y = best[\"df_right\"][:, -1], \n",
    "                    depth = depth +1 \n",
    "                )\n",
    "                \n",
    "                return Node(\n",
    "                    \n",
    "                    feature= best[\"feature_index\"],\n",
    "                    threshold=best['threshold'], \n",
    "                    data_left= best['df_left'], \n",
    "                    data_right = best[\"df_right\"], \n",
    "                    gain = best[\"gain\"]\n",
    "                    \n",
    "                )\n",
    "            \n",
    "        return Node(\n",
    "            \n",
    "            value = Counter(y).most_common(1)[0][0]\n",
    "        )\n",
    "                \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array, features \n",
    "        y: np.array, list, target\n",
    "        return: None\n",
    "        \"\"\"\n",
    "        \n",
    "        self.root = self.build(X, y)\n",
    "                \n",
    "    def predict_single(self, x, tree):\n",
    "        \"\"\"\n",
    "        x: single observation\n",
    "        tree: built tree\n",
    "        return: float, predicted class\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if tree.value != None: \n",
    "            return tree.value \n",
    "        \n",
    "        feature_value = x[tree.feature]\n",
    "        \n",
    "        #explore left\n",
    "        if feature_value <=tree.threshold: \n",
    "            return self.predict_single(x = x, tree= tree.left)\n",
    "        \n",
    "        if feature_value > tree.threshold: \n",
    "            return self.predict_single(x = x, tree = tree.right)\n",
    "        \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        X: np.array, features \n",
    "        return np.array: predicted classes\n",
    "        \n",
    "        \"\"\"\n",
    "        return [self.predict_single(x, self.root) for x in x]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ad1da0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree: \n",
    "    def __init__(self, y, X, min_sample_split, max_depth, depth,\n",
    "                features):\n",
    "        self.y = y \n",
    "        self.X = X\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.max_depth = max_depth\n",
    "        self.features = features\n",
    "        self.n = len(y)\n",
    "        \n",
    "        self.mse_base = np.sum((y-np.mean(y))**2)/len(y)\n",
    "\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def get_mse(ytrue, yhat): \n",
    "        n = len(ytrue)\n",
    "        \n",
    "        residual = ytrue - yhat\n",
    "        residual = residual **2 \n",
    "        residual = sum(residual)\n",
    "        \n",
    "        return residual/n \n",
    "    \n",
    "    @staticmethod\n",
    "    def ma(x, window): \n",
    "        return np.convolve(x, np.ones(window), \"valid\")/window\n",
    "    \n",
    "    \n",
    "    def split(self): \n",
    "        df = self.X.copy()\n",
    "        \n",
    "        df[\"Y\"] = self.y \n",
    "        \n",
    "        mse_base = self.mse_base\n",
    "        best_feature = None\n",
    "        best_value = None\n",
    "        \n",
    "        for feature in self.features:\n",
    "            df_curr = df.dropna().sort_values(feature)\n",
    "            \n",
    "            x_mean = self.ma(df_curr[feature].unique(), 2)\n",
    "            \n",
    "            for value in x_mean: \n",
    "                \n",
    "                left_y = df_curr[df_curr[feature]<value][\"Y\"].values\n",
    "                right_y = df_curr[df_curr[feature]>=value][\"Y\"].values\n",
    "                \n",
    "                left_mean = np.mean(left_y)\n",
    "                right_mean = np.mean(right_y)\n",
    "                \n",
    "                rrs_left = left_y - left_mean \n",
    "                rrs_right = right_y - right_mean \n",
    "                \n",
    "                r = np.concatenate((rrs_left, rrs_right))\n",
    "                \n",
    "                n = len(r)\n",
    "                r = r**2\n",
    "                r = np.sum(r)\n",
    "                mse_split  = r/n\n",
    "                \n",
    "                if mse_split < mse_base: \n",
    "                    mse_base = mse_split\n",
    "                    best_feature = feature\n",
    "                    best_value = value\n",
    "               \n",
    "        return (best_feature, best_value)\n",
    "                    \n",
    "    def build(self):\n",
    "        \n",
    "        df = self.X.copy()\n",
    "        df[\"Y\"] = self.Y\n",
    "        \n",
    "        if (self.depth < self.max_depth) and (self.n>=self.min_sample_split):\n",
    "            \n",
    "            best_feature, best_value  = self.split()\n",
    "            \n",
    "            if best_feature is not None: \n",
    "                \n",
    "                self.best_feature = best_feature\n",
    "                self.best_value = best_value\n",
    "                \n",
    "                left_df = df[df[best_feature]<=best_value].copy()\n",
    "                right_df = df[df[best_feature]>best_value].copy()\n",
    "                \n",
    "                \n",
    "                left = RegressionTree(\n",
    "                    left_df[\"Y\"].values.list(), \n",
    "                    left_df[self.feature],\n",
    "                    depth = self.depth + 1, \n",
    "                    max_depth = self.max_depth, \n",
    "                    min_sample_split = self.min_sample_split, \n",
    "                )\n",
    "                \n",
    "                self.left = left \n",
    "                self.left.build()\n",
    "                \n",
    "                \n",
    "                right = RegressionTree(\n",
    "                \n",
    "                )\n",
    "                \n",
    "                self.right = right \n",
    "                self.right.build()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "24f9f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest: \n",
    "    \n",
    "    \n",
    "    def __init__(self, num_trees = 25, min_sample_split = 2, max_depth =5):\n",
    "        \n",
    "        self.num_trees = num_trees\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.max_depth = max_depth\n",
    "        self.decision = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def _sample(X, y):\n",
    "        \"\"\"\n",
    "        X: np.array, features \n",
    "        y: np.array, target\n",
    "        return tuple (sample of features, sample of target)\n",
    "        \"\"\"\n",
    "        \n",
    "        n_rows, n_cols = X.shape\n",
    "        \n",
    "        sample = np.random.choice(a=n_rows, size=n_rows, replace=True)\n",
    "        return X[bsample], y[sample]\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X:np.array, features \n",
    "        y:np.array, targets\n",
    "        return: None\n",
    "        \"\"\"\n",
    "        \n",
    "        #reset \n",
    "        if len(self.decision)>0: \n",
    "            self.decision = [] \n",
    "            \n",
    "            \n",
    "        num_built = 0 \n",
    "        while num_built < self.num_trees: \n",
    "            try: \n",
    "                \n",
    "                clf = DecisionTree(\n",
    "                    min_sample_split= self.min_sample_split, \n",
    "                    max_depth = max_depth\n",
    "                        \n",
    "                )\n",
    "                \n",
    "                _X, _y = self._sample(X, y)\n",
    "                \n",
    "                clf.fit(_X, _y)\n",
    "                \n",
    "                self.decision.append(clf)\n",
    "                num_built +=1\n",
    "            \n",
    "            except Exception as e: \n",
    "                continue\n",
    "                \n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        param X: np.array \n",
    "        return: None\n",
    "        \n",
    "        \"\"\"\n",
    "        y = []\n",
    "        \n",
    "        for tree in self.decision: \n",
    "            y.append(tree.predict(X))\n",
    "            \n",
    "        prediction = []\n",
    "        \n",
    "        result = Counter(prediction).most_common(1)[0][0]\n",
    "        \n",
    "        return result\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01813b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Decision stump used as weak classifier\n",
    "class DecisionStump():\n",
    "    def __init__(self):\n",
    "        self.polarity = 1\n",
    "        self.feature_idx = None\n",
    "        self.threshold = None\n",
    "        self.alpha = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        X_column = X[:, self.feature_idx]\n",
    "        predictions = np.ones(n_samples)\n",
    "        if self.polarity == 1:\n",
    "            predictions[X_column < self.threshold] = -1\n",
    "        else:\n",
    "            predictions[X_column > self.threshold] = -1\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "class Adaboost():\n",
    "\n",
    "    def __init__(self, n_clf=5):\n",
    "        self.n_clf = n_clf\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize weights to 1/N\n",
    "        w = np.full(n_samples, (1 / n_samples))\n",
    "\n",
    "        self.clfs = []\n",
    "        # Iterate through classifiers\n",
    "        for _ in range(self.n_clf):\n",
    "            clf = DecisionStump()\n",
    "\n",
    "            min_error = float('inf')\n",
    "            # greedy search to find best threshold and feature\n",
    "            for feature_i in range(n_features):\n",
    "                X_column = X[:, feature_i]\n",
    "                thresholds = np.unique(X_column)\n",
    "\n",
    "                for threshold in thresholds:\n",
    "                    # predict with polarity 1\n",
    "                    p = 1\n",
    "                    predictions = np.ones(n_samples)\n",
    "                    predictions[X_column < threshold] = -1\n",
    "                    \n",
    "                    # Error = sum of weights of misclassified samples\n",
    "                    misclassified = w[y != predictions]\n",
    "                    error = sum(misclassified)\n",
    "\n",
    "                    if error > 0.5:\n",
    "                        error = 1 - error\n",
    "                        p = -1\n",
    "\n",
    "                    # store the best configuration\n",
    "                    if error < min_error:\n",
    "                        clf.polarity = p\n",
    "                        clf.threshold = threshold\n",
    "                        clf.feature_idx = feature_i\n",
    "                        min_error = error\n",
    "                        \n",
    "            # calculate alpha\n",
    "            EPS = 1e-10\n",
    "            clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n",
    "\n",
    "            # calculate predictions and update weights\n",
    "            predictions = clf.predict(X)\n",
    "\n",
    "            w *= np.exp(-clf.alpha * y * predictions)\n",
    "            \n",
    "            # Normalize to one\n",
    "            w /= np.sum(w)\n",
    "\n",
    "            # Save classifier\n",
    "            self.clfs.append(clf)\n",
    "            \n",
    "            \n",
    "\n",
    "    def predict(self, X):\n",
    "        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n",
    "        y_pred = np.sum(clf_preds, axis=0)\n",
    "        y_pred = np.sign(y_pred)\n",
    "\n",
    "        return y_pred\n",
    "                        \n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f3095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class \n",
    "\n",
    "\n",
    "class AdaBoost: \n",
    "    \n",
    "    def __init__(self, n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.model = [None]*n_estimators\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        N = len(y)\n",
    "        \n",
    "        #initialize the weights \n",
    "        w = np.array([1/N for i in range(N)])\n",
    "        \n",
    "        \n",
    "        for m in range(self.n_estimators):\n",
    "            \n",
    "            Gm = DecisionTree(max_depth=1).fit(X,y,sample_weight =w) \n",
    "            \n",
    "            err\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41436214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc053d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/master-machine-learning-random-forest-from-scratch-with-python-3efdd51b6d7a\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
